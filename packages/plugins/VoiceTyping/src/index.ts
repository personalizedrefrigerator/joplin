import joplin from 'api';
import { VoiceTypingSessionInfo } from 'api/JoplinVoiceTyping';

// Allow console.*: In mobile plugins, console.log is the way to add Joplin's log file (in debug mode).
/* eslint-disable no-console */

const { loadModule } = require('./vendor/sherpa/sherpa-onnx-wasm-main-asr');
const { createOnlineRecognizer } = require('./vendor/sherpa/sherpa-onnx-asr');

const getLanguageCode = (locale: string) => {
	return locale.substring(0, 2).toLowerCase();
};

const voiceTypingAssets = ({ locale, downloadUrlTemplate = 'http://localhost:8000/' }: VoiceTypingSessionInfo) => {
	const languageCode = getLanguageCode(locale);
	const baseUrl = downloadUrlTemplate.replace(/\{lang\}/g, languageCode);

	const asset = (suffix: string, prefix: string = languageCode) => {
		const fileName = `${prefix}${suffix}`;
		return {
			url: `${baseUrl}/${fileName}`,
			fileName,
		};
	};

	return [
		asset('__sherpa-onnx-wasm-main-asr.data'),
		asset('sherpa-onnx-wasm-main-asr.wasm', ''),
	];
};
type VoiceTypingAssets = ReturnType<typeof voiceTypingAssets>;

const downloadVoiceTypingAssets = async (assets: VoiceTypingAssets) => {
	for (const asset of assets) {
		const cachePath = asset.fileName;
		if (!await joplin.fs.exists(cachePath)) {
			console.info('Fetching', asset.url);
			await joplin.fs.fetchBlob(asset.url, { outputPath: cachePath });
		} else {
			console.log('already exists', asset.url);
		}
	}
};

interface OnlineRecognizerStream {
	acceptWaveform(sampleRate: number, samples: Float32Array);
}

interface RecognizerResult {
	text: string;
}

interface OnlineRecognizer {
	config: {
		modelConfig: {
			paraformer: {
				encoder: string;
			};
		};
	};
	createStream(): OnlineRecognizerStream;
	isReady(stream: OnlineRecognizerStream): boolean;
	decode(stream: OnlineRecognizerStream): void;
	isEndpoint(stream: OnlineRecognizerStream): boolean;
	getResult(stream: OnlineRecognizerStream): RecognizerResult;
	reset(stream: OnlineRecognizerStream): void;
}

type OnTextProcessed = (text: string)=> Promise<void>;
interface ModuleCallbacks {
	onPreview: OnTextProcessed;
	onFinalize: OnTextProcessed;
}

class VoiceTypingModule {
	private recognizerStream_: OnlineRecognizerStream;
	private lastResult_ = '';
	// True if the last output added was a paragraph break
	private lastWasParagraph_ = false;

	private constructor(private recognizer_: OnlineRecognizer, private callbacks_: ModuleCallbacks) {
		this.recognizerStream_ = recognizer_.createStream();
	}

	private static dataFileCache_: string;
	private static async loadDataFile_(assets: VoiceTypingAssets) {
		if (this.dataFileCache_) {
			URL.revokeObjectURL(this.dataFileCache_);
			this.dataFileCache_ = null;
		}

		const asset = assets.find(asset => asset.fileName.endsWith('asr.data'));
		const data = await joplin.fs.readBlob(asset.fileName);
		const url = URL.createObjectURL(data);
		this.dataFileCache_ = url;
		return url;
	}

	private static cachedRecognizers_ = new Map<string, OnlineRecognizer>();
	public static async create(sessionInfo: VoiceTypingSessionInfo, callbacks: ModuleCallbacks) {
		const locale = sessionInfo.locale;
		const cachedRecognizer = this.cachedRecognizers_.get(locale);
		if (cachedRecognizer) {
			return new VoiceTypingModule(cachedRecognizer, callbacks);
		}

		const assets = voiceTypingAssets(sessionInfo);
		const wasm = await joplin.fs.readBlob('./sherpa-onnx-wasm-main-asr.wasm');
		const dataFileUrl = await this.loadDataFile_(assets);

		const { Module, FS } = await loadModule({
			wasmBinary: await wasm.arrayBuffer(),
			locateFile: (f: string) => {
				if (f.includes('sherpa-onnx-wasm-main-asr.data')) {
					return dataFileUrl;
				}
				return f;
			},
		});

		for (const asset of assets) {
			console.log('Transferring asset', asset.fileName);
			// See https://emscripten.org/docs/api_reference/Filesystem-API.html#filesystem-api
			const stream = FS.open(asset.fileName.replace(/^[a-z]{2}_/, ''), 'w+');
			const array = new Uint8Array(await (await joplin.fs.readBlob(asset.fileName)).arrayBuffer());
			FS.write(stream, array, 0, array.length, 0);
			FS.close(stream);
			console.log('Transferred asset', asset.fileName);
		}

		const recognizer = createOnlineRecognizer(Module);
		this.cachedRecognizers_.set(locale, recognizer);
		return new VoiceTypingModule(recognizer, callbacks);
	}

	// See https://huggingface.co/spaces/Banafo/Kroko-Streaming-ASR-Wasm/blob/main/app-asr.js
	// and the default app-asr.js generated by sherpa-onnx.
	public async onAudio(audio: Float32Array, sampleRate: number) {
		if (sampleRate !== 16000) {
			throw new Error(`Unsupported audio sample rate ${sampleRate}`);
		}

		this.recognizerStream_.acceptWaveform(sampleRate, audio);
		while (this.recognizer_.isReady(this.recognizerStream_)) {
			this.recognizer_.decode(this.recognizerStream_);
		}

		const result = this.recognizer_.getResult(this.recognizerStream_).text;
		if (result.length) {
			this.lastResult_ = result.trim();
		}

		const isEndpoint = this.recognizer_.isEndpoint(this.recognizerStream_);
		if (isEndpoint) {
			if (this.lastResult_.length) {
				await this.callbacks_.onFinalize(this.lastResult_);
				this.lastWasParagraph_ = false;
				this.lastResult_ = '';
			} else if (!this.lastWasParagraph_) {
				this.lastWasParagraph_ = true;
				await this.callbacks_.onFinalize('\n\n');
			}
			await this.callbacks_.onPreview('');
			this.recognizer_.reset(this.recognizerStream_);
		} else {
			await this.callbacks_.onPreview(this.lastResult_);
		}
	}
}

joplin.plugins.register({
	onStart: async function() {
		const isDownloaded = async (assets: VoiceTypingAssets) => {
			for (const asset of assets) {
				if (!await joplin.fs.exists(asset.fileName)) {
					return false;
				}
			}
			return true;
		};

		let recognizer: VoiceTypingModule|null = null;
		await joplin.voiceTyping.registerProvider({
			name: 'Kroko',
			id: 'kroko',
			attribution: {
				url: 'https://huggingface.co/Banafo/Kroko-ASR',
				libraryName: 'Kroko',
			},
			supportedLanguages: ['en', 'fr'],
			async download(info: VoiceTypingSessionInfo) {
				await downloadVoiceTypingAssets(voiceTypingAssets(info));
			},
			async isDownloaded(info) {
				return await isDownloaded(voiceTypingAssets(info));
			},
			async canUpdateModel(info) {
				return !await isDownloaded(voiceTypingAssets(info));
			},
			async clearCache(info) {
				for (const asset of voiceTypingAssets(info)) {
					await joplin.fs.remove(asset.fileName);
				}
			},
			async onStart(sessionId, info) {
				recognizer = await VoiceTypingModule.create(info, {
					onPreview: (text) => {
						if (!recognizer) {
							console.log('Session closed');
							return;
						}
						return joplin.voiceTyping.updateRecognitionPreview(sessionId, text);
					},
					onFinalize: (text) => {
						// Session closed
						if (!recognizer) return;

						return joplin.voiceTyping.onTextRecognised(sessionId, text);
					},
				});

				void (async () => {
					while (true) {
						const audioData = await joplin.voiceTyping.nextAudioData(sessionId, 3);
						if (!audioData || audioData.data.length === 0) return;
						await recognizer.onAudio(audioData.data, audioData.sampleRate);
					}
				})();
			},
			async onStop(sessionId) {
				console.log('Stopped voice typing session', sessionId);
				recognizer = null;
			},
		});

	},
});
